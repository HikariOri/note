# 线性代数与矩阵分析基础

## Vector Space

<div class="math-env definition" data-name="vector space">

A *vector space* is a set $V$ along with an addition on $V$ and a scalar multiplication on $V$ such that the following properties hold.

+ **commutativity**: $u+v=v+u$ for all $u,v \in V$.
+ **associativity**: $\left( u+v \right)+w=u+\left( v+w \right)$ and $\left( ab \right)v=a\left( bv \right)$ and for all $a,b \in \mathbb{F}$.
+ **additive identity**: There exists element $0\in V$ such that $v+0=v$ for all $v\in V$.
+ **additive inverse**: For every $v\in V$, there exists $w\in V$ such that $v+w=0$.
+ **multiplicative identity**: $1v=v$ for all $v\in V$.
+ **distributive properties**: $a\left( u+v \right)=au+av$ and $\left( a+b \right)v=av+bv$ for all $a,b \in \mathbb{F}$ and all $u,v \in V$.

</div>

<div class="math-env remark">

$\mathbb{F}$ 一般取 $\mathbb{R}$ 或 $\mathbb{C}$

</div>


<div class="math-env definition" data-name="线性组合">

$V$ 中的向量组 $\boldsymbol{v}_1,\boldsymbol{v}_2,\cdots,\boldsymbol{v}_m$ 的线性组合是形如

$$
a_1 \boldsymbol{v}_1+a_2 \boldsymbol{v}_2+\cdots+a_m \boldsymbol{v}_m
$$

的向量，其中 $a_1,a_2,\cdots,a_n \in \mathbb{F}$.

</div>

<div class="math-env definition" data-name="线性无关/线性相关（Linearly dependent/independent）"）>

对于向量集合 $\left\{ \boldsymbol{u}_1, \boldsymbol{u}_2,\cdots,\boldsymbol{u}_n \right\}$，线性相关指的是，在 $\mathbb{F}$ 中存在不全为 $0$ 的一组数 $a_1,a_2,\cdots,a_n$ 使得：

$$
a_1 \boldsymbol{u}_1+a_2 \boldsymbol{u}_2+\cdots a_n\boldsymbol{u}_n=0
$$

即：

$$
\boldsymbol{u}_1=-\frac{a_2}{a_1}\boldsymbol{u}_2-\frac{a_3}{a_1}\boldsymbol{u}_3-\cdots-\frac{a_n}{a_1}\boldsymbol{u}_n\,\,\,\,(a_1\neq 0)
$$

说明 $\boldsymbol{u}_1$ 是 $\boldsymbol{u}_2,\boldsymbol{u}_3,\cdots,\boldsymbol{u}_n$ 的线性组合.

线性无关：$\mathbb{F}$ 中不存在不全为 $0$ 的这样的一组数.

</div>


### 向量空间的维数

<div class="math-env definition" data-name="维数（dimension）">

- 有限维向量空间的维度是这个向量空间中任意一个基的长度.
- 有限维向量空间 $V$ 的维数记为 $\operatorname{dim}V$.

</div>

<div class="math-env remark">

向量空间中任意一个基的长度实际上就是这个向量空间中能找到的线性无关的向量个数的最大值.

</div>

$\operatorname{dim}V$ 个线性无关的向量构成空间的一组基矢（basis vectors）.

- 任意空间中的向量可以唯一地表示为这组基矢的线性组合（反证法给出唯一性）.
- 线性组合的系数（coefficients）称为该矢量在这组基下的坐标（coordinates）.

### 图形学研究的维度

### 低维度向量和向量空间（2D~3D）

- 物理空间：Mesh、曲线、点云的坐标及导数.
  - 欧几里得空间（$x,y,z$）
  - 闵可夫斯基空间（$x,y,z,\mathrm{i}ct$）
  - 颜色空间：RGB, CMYK
- 高纬向量和向量空间
  - 灰度数字图像上所有像素值组成的向量.
    - $1920\times 1080$ 的灰度数字图像维度高达 $200$ 万.
  - 二维或三维图形的所有自由度组成的向量
    - 《原神》中纳西妲运动的自由度为 $45459$
    - SIGGRAPH 水体模拟求解的向量维度一般在 $10^6\sim 10^7$.

## 线性映射（Linear Mapping）

- $f:V\to W$
  - $V,W$均为向量空间
  - $f\left( \boldsymbol{u}+\boldsymbol{v} \right) =f\left( \boldsymbol{u} \right)+f\left( \boldsymbol{v} \right)$
  - $f\left( \alpha \boldsymbol{v} \right)=\alpha f\left( \boldsymbol{v} \right)$
- 推论
  - $f\left( \boldsymbol{0} \right)=\boldsymbol{0}$
  - $f\left( a \boldsymbol{u}+b \boldsymbol{v} \right)=af\left( \boldsymbol{u} \right)+bf\left( \boldsymbol{v} \right)$
- 低维空间的线性映射
  - 缩放、旋转是线性映射
  - 平移不是线性映射
  - 仿射变换（affine transformation）$=$ 缩放、旋转+平移

## 矩阵（Matrix）

- 什么是矩阵
  - 形式上：二维数组阵列（2D array）.
  - 意义上：线性映射的一种表示
- 矩阵运算的意义
  - 矩阵与向量的乘法：给出向量在新的空间（经过矩阵对应的线性变换后的空间）里的坐标
  - 矩阵的乘法：对空间的多次变换的复合
- 仿真
  - $n\times n$的矩阵，暗示变换前后空间的维度相同：
- 单位矩阵一般记为 $\mathbf{E}$ 或 $\mathbf{I}$

## 矩阵单目运算

- 转置（Transpose）$\mathbf{A}^{\mathrm{T}}$：$\mathbf{A}$ 的所有元素的下标的行、列互换.
  - 意义：矩阵对应的线性变换在对偶空间里的逆变换对应的矩阵.
  - 性质：$\left( \mathbf{A}\mathbf{B} \right)^{\mathrm{T}}=\mathbf{B}^{\mathrm{T}}\mathbf{A}^{\mathrm{T}}$
- 行列式（Determinant）：$\det \mathbf{A}=\left| \mathbf{A} \right|$
  - 意义：矩阵对应的线性变换对空间的拉伸程度的度量（物体经过变换前后的体积比）
  - 定义：在 $n$ 阶方阵中选 $n$ 个元素使得每行每列各有一个元素被选出，求其乘积，再将不同的选取方案乘以 $\pm 1$ （由选取翻案的奇偶性决定）加和
  - 计算方法
    - 低维矩阵：$n$ 条主对角线分别计算乘积的和，减去 $n$ 条次对角线分别计算乘积的和
    - 高维矩阵：高斯消元法后取对角线元素的积
  - 性质：转置不变，即 $\det \mathbf{A}^{\mathrm{T}}=\det \mathbf{A}$；交换行（列）取反
- 迹（Trace）$\operatorname{trace} \mathbf{A}$：矩阵的对角线元素之和
  - 意义：矩阵的特征值之和
  - 性质
    - $\operatorname{trace}\mathbf{A}=\operatorname{trace}\left( \mathbf{A}^{\mathrm{T}} \right)$
    - $\operatorname{trace} \mathbf{A}\mathbf{B}=\operatorname{trace} \mathbf{B}\mathbf{A}$
    - $\operatorname{trace}\left( \mathbf{A}+\mathbf{B} \right)=\operatorname{trace}\left( \mathbf{B}+\mathbf{A} \right)$
- 逆（INversed）$\mathbf{A}^{-1}$：满足 $\mathbf{A}\mathbf{A}^{-1}=\mathbf{A}^{-1}\mathbf{A}=1$ 的矩阵
  - 求解：伴随矩阵法；高斯消元法
  - 性质
    - $\left( \mathbf{A}\mathbf{B} \right)^{-1}=\mathbf{B}^{-1}\mathbf{A}^{-1}$
    - $\left| \mathbf{A} \right|^{-1}\left| \mathbf{A} \right|=\left| \mathbf{A} \right|\left| \mathbf{A} \right|^{-1}=\mathbf{E}$
- 伴随（Adjugated）$\mathbf{A}^{*}$：由 $\mathbf{A}$ 的每个元素的代数余子式 $A_{ij}$ 构成的矩阵
  - 代数余子式 $A_{ij}=\left( -1 \right)^{i+j}M_{ij}$
  - $M_{ij}$ 为余子式，即删除第 $i$ 行第 $j$ 列后的行列式的值
  - 性质：$\mathbf{A}\mathbf{A}^{*}=\mathbf{A}^{*}\mathbf{A}=\left| \mathbf{A} \right|\mathbf{E}$

## 特征值（Eigenvalues）

对于某些向量 $\boldsymbol{u}$ 满足 $\lambda \boldsymbol{u}=\mathbf{A}\boldsymbol{u}$ 的数 $\lambda$ 称为特征值，其中 $\mathbf{A}$ 为 $n$ 阶方阵

这些向量 $\boldsymbol{u}$ 称为特征向量，每个特征值对应的特征向量构成向量子空间（$\lambda \boldsymbol{u}=\mathbf{A}\boldsymbol{u},\lambda \boldsymbol{v}=\mathbf{A}\boldsymbol{v}\implies \lambda\left( \boldsymbol{u}+\boldsymbol{v} \right)=\mathbf{A}\left( \boldsymbol{u}+\boldsymbol{v} \right)$）

求解特征值：$\left( \lambda \mathbf{E}-\mathbf{A} \right)\boldsymbol{u}=0$

考虑关于 $\boldsymbol{u}$ 的非零解，则 $\left| \lambda \mathbf{E}-\mathbf{A} \right|=0$，该行列式对应于一个一元 $n$ 次方程

特征值的意义：对应某些向量，特定线性变换的作用效果与数乘等价（也可以说成对于一个线性变换，作用于特定向量的效果与数乘等价）

### 矩阵多项式的特征值

对于矩阵多项式：

$$
f\left( \mathbf{A} \right)=c_k\mathbf{A}^{k}+c_{k-1}\mathbf{A}^{k-1}+\cdots+c_1\mathbf{A}+\mathbf{c_0E}
$$

若 $\lambda$ 是 $\mathbf{A}$ 的特征值，则 $f\left( \lambda \right)$ 为 $f\left( \mathbf{A} \right)$ 的特征值

设 $\mathbf{A}$ 的特征值为：$\left\{ \lambda_1,\lambda_2,\cdots,\lambda_n\right\}$，则 $f\left( \mathbf{A} \right)$ 的全部特征值由 $\left\{ f\left(\lambda_1  \right),f\left(\lambda_2  \right),\cdots,f\left(\lambda_n  \right)\right\}$ 给出

重要应用：最大特征值成为谱半径（spectral radius），最大最小特征值之比称为条件数

## 赋范向量空间（Normaed Vector Space）

### 度量空间

向量空间中的元素不能比大小

对于集合 $V$，定义度量函数 $d: V\times V\to \mathbb{R}$，设 $x,y,z \in V$
- 非负性：$d\left( x,y \right)\geqslant 0$
- 定性（不可区分者的同一性）：$d\left( x,y \right)=0\Leftrightarrow x=y$
- 对称性：$d\left( x,y \right)=d\left( y,x \right)$
- 三角不等式：$d\left( x,z \right)\leqslant d\left( x,y \right)+d\left( y,z \right)$

定义了度量函数的集合 $V$ 成为度量空间（metric space），度量函数 $d$ 又称为「距离」

### 赋范向量空间

<div class="math-env definition" data-name="赋范向量空间">

如果一个空间既是向量空间，又是度量空间，则称为赋范向量空间

</div>

<div class="math-env remark">

$d\left( \boldsymbol{u},\boldsymbol{v} \right)=\left\| \boldsymbol{u}-\boldsymbol{v} \right\|$

</div>

<div class="math-env definition" data-name="范数">

$\left\| \cdot \right\|:V\to \mathbb{R}$，设 $\boldsymbol{u},\boldsymbol{v}\in V$
- 非负性：$\left\| \boldsymbol{u} \right\|\geqslant 0$
- 正定性：$\left\| \boldsymbol{u} \right\|=0\Leftrightarrow \boldsymbol{u}=\boldsymbol{0}$
- 正齐次性：$\left\| a \boldsymbol{u} \right\|=\left| a \right|\left\| \boldsymbol{u} \right\|(a\in \mathbb{R})$
- 次可加性、三角不等式：$\left\| \boldsymbol{u}+\boldsymbol{v} \right\|\leqslant \left\| \boldsymbol{u} \right\|+\left\| \boldsymbol{v} \right\|$

</div>

## 内积空间（Inner Product Space）

内积：$V\times V\to \mathbb{R}$

- $\left<\boldsymbol{u},\boldsymbol{v} \right> =\overline{\left<\boldsymbol{u},\boldsymbol{v} \right>}$
- $\left<\boldsymbol{u},\boldsymbol{v}+\boldsymbol{w} \right> =\left<\boldsymbol{u},\boldsymbol{v}\right>+\left<\boldsymbol{u},\boldsymbol{w} \right>$
- $\left<\boldsymbol{u},a \boldsymbol{v} \right> =\overline{a}\left<\boldsymbol{u},\boldsymbol{v} \right>$
- $\left<\boldsymbol{u},\boldsymbol{u} \right> \geqslant 0$
- 赋范向量空间 vs. 内积空间
  - $d\left( \boldsymbol{u},\boldsymbol{u} \right)=\left<\boldsymbol{u},\boldsymbol{u} \right>$
  - 范数只给出了向量的长度
  - 内积还给出了向量的夹角

<figure id="fig:vector_space">
    <img src="https://raw.githubusercontent.com/HikariOri/image/main/20250627164056788.png" alt="向量空间"/>
    <figcaption>线性空间、度量空间、赋范线性空间和内积空间的关系</figcaption>
</figure>

## 内积与正交

### 正交基底与单位正交基底

<div class="math-env definition" data-name="两向量夹角">

$$
\theta_{\boldsymbol{u}\boldsymbol{v}}=\arccos \frac{\left<\boldsymbol{u},\boldsymbol{v} \right>}{\sqrt{\left<\boldsymbol{u},\boldsymbol{u} \right>\left<\boldsymbol{v},\boldsymbol{v} \right>}}
$$

当 $\cos\theta_{\boldsymbol{u}\boldsymbol{v}}=0$ 时，称这两个向量正交.

</div>

<div class="math-env definition" data-name="正交基底">

一组两两之间相互正交的矢量基底称为正交基底.

</div>

<div class="math-env definition" data-name="单位基底">

模长均为 $1$ 的向量基底称为单位基底.

</div>

通过施密特正交化加规范化可以将任意一组基底变成单位正交基底

单位正交基底的性质（设 $\boldsymbol{u}=u_1 \boldsymbol{e}_1+u_2 \boldsymbol{e}_2+u_3 \boldsymbol{e}_3,\boldsymbol{v}=v_1 \boldsymbol{e}_1+v_2 \boldsymbol{e}_2+v_3 \boldsymbol{e}_3$）

$$
\left<\boldsymbol{u},\boldsymbol{v} \right>= \left< u_1 \boldsymbol{e}_1+u_2 \boldsymbol{e}_2+u_3 \boldsymbol{e}_3,v_1 \boldsymbol{e}_1+v_2 \boldsymbol{e}_2+v_3 \boldsymbol{e}_3\right> = u_1 v_1+u_2v_2+u_3v_3
$$

单位正交变换：不改变任意两个向量的内积的变换（保持单位正交基底）

单位正交矩阵：单位正交变换的矩阵 $\mathbf{R}$，满足 $\mathbf{R}^{\mathrm{T}}=\mathbf{R}^{-1}$；对应于旋转于镜像空间内的旋转；对应于旋转与镜像空间内的旋转

笛卡尔坐标系（Cartesian coordinates）：一组 $\mathbb{R}^n$ 中的单位正交基底加上原点构成的坐标系；$\left<\boldsymbol{u},\boldsymbol{v} \right>=\boldsymbol{u}^{\mathrm{T}}\boldsymbol{v}=\boldsymbol{v}^{\mathrm{T}}\boldsymbol{u}$

## 幺正空间与幺正变换

幺正（unitay）空间，数学上又译为酉矩阵：定义了内积的复数域 $\mathbb{C}$ 上的线性空间，内积通过埃尔特米（Hermite）函数（$\left<\boldsymbol{u},\boldsymbol{v} \right>\sum_{i=0}^{n} u_iv_i$）给出

幺正变换，数学上有译为酉变换
- 将幺正空间中的单位正交基底变换为单位正交基底的变换
  - 单位正交基：亦称为规范正交基、标准正交基
- 单位正交基可视为幺正变换在实数域 $\mathbb{R}$ 上的特例
- 幺正空间/变换的应用
  - 量子力学：波函数在复数域上的定义
  - 幺正性：算子（operator）保内积
  - 线性算子：无限维空间（例如函数空间）上的线性变换，也存在特征值、内积等概念
  
## 低维变换矩阵

- 2D 线性变换
  - $\displaystyle \begin{bmatrix}a_{11} & a_{12} \\a_{21} & a_{22} \end{bmatrix} \begin{bmatrix}x \\y  \end{bmatrix}=\begin{bmatrix} a_{11}x+a_{12}y\\a_{21}x+a_{22}y \end{bmatrix}$
  - $\operatorname{scale}\left( s_x,s_y \right)=\begin{bmatrix}s_x & 0 \\0 & s_y \end{bmatrix}$
  - $\operatorname{shear}_x\left( s \right)=\begin{bmatrix}1 & s \\0 & 1 \end{bmatrix},\operatorname{shear}_y\left( s \right)=\begin{bmatrix}1 & 0 \\s & 1 \end{bmatrix}$
  - $\operatorname{rotate}\left( \phi \right)=\begin{bmatrix}\cos \phi & -\sin \phi \\\sin \phi &  \cos \phi\end{bmatrix}$

绕任意轴的旋转：

$$
\mathbf{R}^{\mathrm{T}}\begin{bmatrix}
\cos \phi & -\sin\phi & 0 \\
\sin\phi &\cos\phi  &0  \\
0 & 0 &  1
\end{bmatrix}\mathbf{R}
$$

其中：$\mathbf{R}$ 为将旋转轴转为 $z$ 轴的变换矩阵

## 齐次坐标（Homogeneous Coordinates）

实质：用 $n+1$ 个数表示 $n$ 为坐标

2D:

$$
\left( x,y,z \right)\to \left( \frac{x}{w},\frac{y}{w} \right)
$$

3D:

$$
\left( x,y,z,w \right)\to \left( \frac{x}{w},\frac{y}{w},\frac{z}{w} \right)
$$

齐次坐标等比例缩放不影响其表示的 $n$ 维坐标

2D 仿射变换：

$$
\begin{bmatrix}
a_{11} & a_{12} & t_x \\
a_{21} & a_{22} & t_y \\
0 & 0 & 1 
\end{bmatrix}\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}
=\begin{bmatrix} a_{11}x+a_{12}y+t_x \\ a_{21}x+a_{22}y+t_y \\ 1 \end{bmatrix}
$$

平移变换一般在线性变换之后执行

默认顺序（Houdini、Blender 等）：缩放->旋转->平移

<figure id="fig:20250628225857720"><img src="https://raw.githubusercontent.com/HikariOri/image/main/20250628225857720.png" alt="20250628225857720"/><figcaption>齐次坐标等比例缩放</figcaption></figure>

## 矩阵的变换

矩阵是线性变换的表示，所以矩阵的变换其实是线性变换的「变换」。

### 相似变换

存在可逆矩阵 $\mathbf{P}$ 使得 $\mathbf{P}^{-1}\mathbf{A}\mathbf{P}=\mathbf{B}$，称矩阵 $\mathbf{A},\mathbf{B}$ 相似，记作 $\mathbf{A}\sim \mathbf{B}$.

意义：$\mathbf{A}$ 与 $\mathbf{B}$ 可以看作同一个线性变换在不同基底下的表象（一个线性变换的矩阵与基底的选取有关）。

相似对角化：当存在 $n$ 个线性无关的特征向量时，可以将矩阵相似为一个对角矩阵，这说明：在某个特殊的基底下，矩阵对应的线性变换等价于纯缩放变换。

### 合同变换

存在可逆矩阵 $\mathbf{C}$ 使得 $\mathbf{C}^{\mathrm{T}}\mathbf{A}\mathbf{C}=\mathbf{B}$，称矩阵 $\mathbf{A},\mathbf{B}$ 合同，记作 $\mathbf{A} \approx \mathbf{B}$

意义：$\mathbf{A}$ 与 $\mathbf{B}$ 可以看作同一个二次型在不同基底下的表象。

## 二次型

### 二次型（Quadra Form）：$n$ 个变量的二次多项式

示例：圆锥曲线 $Ax^2+2Bxy+Cy^2+2Dx+2Ey+F=0$，矩阵表示：

$$
\begin{bmatrix} x, y, 1 \end{bmatrix}\begin{bmatrix}
A & B & D \\
B & C & E \\
D & E & F 
\end{bmatrix}\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}=0
$$

二次型一般形式：$\boldsymbol{x}^{\mathrm{T}}\mathbf{A}\boldsymbol{x}=0$

合同变换后：

$$
\boldsymbol{x}^{\mathrm{T}}\left( \mathbf{C}^{\mathrm{T}}\mathbf{A}\mathbf{C} \right)\boldsymbol{x}\implies \boldsymbol{x}^{\mathrm{T}}\mathbf{C}^{\mathrm{T}}\mathbf{A}\mathbf{C}\boldsymbol{x}=0\implies \left( \mathbf{C}\boldsymbol{x} \right)^{\mathrm{T}}\mathbf{A}\left( \mathbf{C}\boldsymbol{x} \right)=0
$$

意义：变换基底后的二次型系数

应用：将任意朝向的椭圆方程转换为标准形式 $\displaystyle \frac{x^2}{a^2}+\frac{y^2}{b^2}=1$

$$
\begin{bmatrix} x,y,1\end{bmatrix}\begin{bmatrix}
\frac{1}{a^2} & 0 & 0 \\
0 & \frac{1}{b^2} & 0 \\
0 & 0 &  -1
\end{bmatrix}\begin{bmatrix} x \\ y \\1 \end{bmatrix}=0
$$

## 正定矩阵与对称矩阵

### 正定矩阵（Positive-definite Matrix）

定义：满足 $\boldsymbol{x}^{\mathrm{T}}\mathbf{A}\boldsymbol{x}>0$（$\boldsymbol{x}\neq 0$ 且 $\boldsymbol{x}$ 为满足上面乘法的任意向量）的矩阵

意义：可以构建恒取正值的二次型

例如：二元可微函数 $f\left( x,y \right)$，其二级微分

$$
\begin{bmatrix} \mathrm{d}x,\mathrm{d}y \end{bmatrix}\begin{bmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x\partial y} \\
\frac{\partial^2 f}{\partial x\partial y} & \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}\begin{bmatrix} \mathrm{d}x \\ \mathrm{d}y \end{bmatrix}
$$

给出沿任意方向的二阶导数。

性质：正定矩阵的特征值全为正数（逆命题不成立）

半正定矩阵：满足 $\boldsymbol{x}^{\mathrm{T}}\mathbf{A}\boldsymbol{x}\geqslant 0$ 的矩阵

### 对称矩阵

实对称矩阵

定义：$\mathbb{R}$ 上的对称矩阵（$\mathbf{A}^{\mathrm{T}}=\mathbf{A}$）

性质：
- 可以被正交矩阵对角化
- 只要特征值全为整数就是正定的

$\mathbb{F}$ 的埃尔米特（Hermitian）矩阵满足：$\mathbf{A}^{\mathrm{H}}=\mathbf{A}$ 

## 矩阵分解（Decomposition）

### PLU 分解

$\mathbf{A}=\mathbf{P}\mathbf{L}\mathbf{U}$

- 要求：方阵
- $\mathbf{P}$ 为初等变换矩阵，$\mathbf{L}$ 为下三角矩阵，$\mathbf{U}$ 为上三角矩阵
- 当 $\mathbf{P}$ 取单位矩阵时则退化为 $\mathbf{L}\mathbf{U}$ 分解
- 应用：高斯消元法求解矩阵方程

<figure id="fig:20250628232818517"><img src="https://raw.githubusercontent.com/HikariOri/image/main/20250628232818517.png" alt="20250628232818517"/><figcaption>矩阵的 PLU 分解</figcaption></figure>

### 乔里斯基（Cholesky）分解

$\mathbf{A}=\mathbf{L}\mathbf{L}^{\mathrm{H}}$

- 要求：方阵、正定、实对称/埃尔米特
- $\mathbf{L}$ 为下三角矩阵
- 应用：高效求解（稀疏）矩阵方程

<figure id="fig:20250628233054162"><img src="https://raw.githubusercontent.com/HikariOri/image/main/20250628233054162.png" alt="20250628233054162"/><figcaption>矩阵的 Cholesky 分解</figcaption></figure>

### QR 分解

$\mathbf{A}=\mathbf{Q}\mathbf{R}$

- 要求： $\mathbf{A}_{m\times n}$ 列满秩，$m\geqslant n$
- $\mathbf{Q}_{m\times m}$ 为幺正矩阵，$R_{m\times n}$ 为上三角矩阵
- 非方阵时 $\mathbf{R}$ 的下半部分为零矩阵
- 求解：施密特正交化、吉文斯或豪斯霍尔变换
- 应用：高效求解（稠密）矩阵方程；分解形变梯度

<figure id="fig:20250628233251951"><img src="https://raw.githubusercontent.com/HikariOri/image/main/20250628233251951.png" alt="20250628233251951"/><figcaption>矩阵的 QR 分解</figcaption></figure>

### 奇异值分解

$\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\mathrm{H}}$

- 要求：$\mathbf{A}_{m\times n}$
- $\mathbf{U}_{m\times m}$ 为幺正矩阵，$\mathbf{V}_{n\times n}$ 为幺正矩阵，$\mathbf{\Sigma}_{m\times n}$ 为对角矩阵
- 对角矩阵中的元素称为奇异值，均为非负
- 若 $m=n$，则有 $\sqrt{\mathbf{A}^{\mathrm{H}}\mathbf{A}}=\sqrt{\mathbf{V}\mathbf{\Sigma}^2\mathbf{V}^{\mathrm{H}}}=\mathbf{V}\left| \Sigma \right|\mathbf{V}^{\mathrm{H}}$
- 当 $\mathbf{A}$ 是实对称正定矩阵时，特征值与奇异值重合

<figure id="fig:20250628233624996"><img src="https://raw.githubusercontent.com/HikariOri/image/main/20250628233624996.png" alt="20250628233624996"/><figcaption>矩阵的奇异值分解</figcaption></figure>

## 向量与矩阵的范数

### 向量范数

设 $\boldsymbol{x}$ 维度为 $n\times 1$

- $L_0$ 范数：$\left\| \boldsymbol{x} \right\|_0=\sum_{i=1}^n\left[ x_i\neq 0 \right]$
- $L_1$ 范数：$\left\| \boldsymbol{x} \right\|_1=\sum_{i=1}^n\left| x_i \right|=\left| x_1 \right|+\left| x_2 \right|+\cdots+\left| x_n \right|$
- $L_2$ 范数：$\left\| \boldsymbol{x} \right\|_2=\sqrt{\sum_{i=1}^n x_i^2}=\left( x_1^2+x_2^2+\cdots+x_n^2 \right)$（亦称 Euclidean/Frobenius 范数） 
- $L_p$ 范数：$\displaystyle \left\| x \right\|_p=\left( \left| x_1 \right|^p+\left| x_2 \right|^p+\cdots+\left| x_n \right|^p \right)^\frac{1}{p}\left( p\geqslant 1 \right)$
- $L_{\infty}$ 范数：$\left\| x \right\|_{\infty}=\max\left\{ \left| x_1 \right|,\left| x_2 \right|,\cdots,\left| x_n \right| \right\}$

<div class="math-env remark">

$L_0$ 范数不满足正定性、正齐次性、三角不等式，不能构成度量函数，本质上不是范数，不可导，作为优化目标时性能非常底下

</div>

### 矩阵范数

设 $\mathbf{A}$ 维度为 $m\times n$

#### 诱导范数（Induced Norm）

$$
\left\| \mathbf{A} \right\|=\max\left\{ \left\| \mathbf{A}\boldsymbol{x} \right\|:\boldsymbol{x}\in \mathbb{R}^n,\left\| \boldsymbol{x} \right\|=1 \right\}=\max\left\{ \frac{\left\| \mathbf{A}\boldsymbol{x} \right\|}{\left\| \boldsymbol{x} \right\|}:\boldsymbol{x}\in \mathbb{R}^n,\left\| \boldsymbol{x} \right\|=0 \right\}
$$

由向量的 $L_p$ 范数可以得到矩阵的诱导 $L_p$ 范数：

$$
\left\| \mathbf{A} \right\|_p=\max\left\{ \frac{\left\| \mathbf{A}\boldsymbol{x} \right\|_p}{\left\| \boldsymbol{x} \right\|_p}:\boldsymbol{x}\in \mathbb{R}^n,\left\| \boldsymbol{x} \right\|\neq 0\right\}
$$

几何意义：$\mathbf{A}$ 转为线性变换所产生的对向量模长的最大放大倍数

诱导范数的相容性：$\left\| \mathbf{A}\boldsymbol{x} \right\|\leqslant \left\| \mathbf{A} \right\|\left\| \boldsymbol{x} \right\|$，证明如下：

$$
\frac{\left\| \mathbf{A}\boldsymbol{x} \right\|}{\left\| \boldsymbol{x} \right\|}\leqslant \max\left\{ \frac{\left\| \mathbf{A}\boldsymbol{x} \right\|}{\left\| \boldsymbol{x} \right\|}:\boldsymbol{x}\in \mathbb{R}^n,\left\| \boldsymbol{x} \right\| \neq 0\right\}=\left\| \mathbf{A} \right\|
$$

- $\displaystyle \left\| \mathbf{A} \right\|_1=\max_{1\leqslant j\leqslant n}\sum_{i=1}^m \left| a_{ij} \right|$（$\mathbf{A}$ 的所有列向量最大绝对值和）
- $\left\| \mathbf{A} \right\|_2=\sigma_{\operatorname{max}}\left( \mathbf{A} \right)$（$\mathbf{A}$ 的最大奇异值，故该范数又称为谱范数）
- $\displaystyle \left\| \mathbf{A} \right\|_{\infty}=\max_{1\leqslant i\leqslant m}\sum_{j=1}^n \left| a_{ij} \right|$（$\mathbf{A}$ 的所有行向量最大绝对值和）

#### 元素形式范数（Entrywise Norm）

将矩阵降维成向量并使用向量的 $L_p$ 范数不满足正定性

- $\displaystyle \left\| \mathbf{A} \right\|_1=\sum_{i=1}^m\sum_{j=1}^n\left| a_{ij} \right|$（和范数、$L_1$ 范数）
- $\displaystyle \left\| \mathbf{A} \right\|_{\operatorname{F}}=\left( \sum_{i=1}^m\sum_{j=1}^n a_{ij}^2 \right)^{\frac{1}{2}}=\operatorname{trace}\mathbf{A}^{\mathrm{T}}\mathbf{A}$（Frobenius 范数、Euclidean 范数、Schur 范数、$L_2$ 范数）
- $\displaystyle  \left\| \mathbf{A} \right\|_{\infty}=\max_{1\leqslant i\leqslant m}\max_{1\leqslant j\leqslant n}\left| a_{ij} \right|$（最大范数、$L_{\infty}$ 范数）

#### 沙滕（Schatten）范数：一种不随幺正变换而改变的范数

- $\displaystyle \left\| \mathbf{A} \right\|_p=\left( \sum_{i=1}^{\max\left\{ m,n \right\}}\sigma^p_i \right)$（$\sigma_i$ 为奇异值）
- $\left\| \mathbf{A} \right\|_1$ 为奇异值之和，称为核范数（nuclear norm）
- $\left\| \mathbf{A} \right\|_2=\left\| \mathbf{A} \right\|_{\operatorname{F}}$
- $\left\| \mathbf{A} \right\|_\infty$ 等价于谱范数

## 矩阵求导

矩阵求导再图形学中的应用
- 伴随方法中求物理量相对初始参数的导数（深度学习中求输出相对于输入的导数）
- 软体仿真中给定势能函数求应力张量

### 布局

分子布局：分母装置

$$
\frac{\partial \boldsymbol{f}_{2\times 1}\left( \boldsymbol{x} \right)}{\partial \boldsymbol{x}^{\mathrm{T}}_{3\times 1}}=
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \frac{\partial f_1}{\partial x_3} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \frac{\partial f_2}{\partial x_3} 
\end{bmatrix}_{2\times 3}
$$

分母布局：分子布局

$$
\frac{\partial \boldsymbol{f}^{\mathrm{T}}_{2\times 1}\left( \boldsymbol{x} \right)}{\partial \boldsymbol{x}_{3\times 1}}=\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_2}{\partial x_1} \\
\frac{\partial f_1}{\partial x_2} & \frac{\partial f_2}{\partial x_2} \\
\frac{\partial f_1}{\partial x_3} & \frac{\partial f_2}{\partial x_3} \\
\end{bmatrix}
$$

从本质上说，$m$ 个变元的矩阵对 $n$ 个变元的矩阵求导，所得结果为 $m\times n$ 个变元。所谓的布局，决定了结果的变元按什么规则排列。

### 向量变元的实值标量函数 $f\left( \boldsymbol{x} \right)$

设 $\boldsymbol{x}_{n\times 1}=\left[ x_1,x_2,\cdots,x_n \right]$，则有

$$
\nabla_{\boldsymbol{x}}f\left( \boldsymbol{x} \right)
=\frac{\partial f\left( \boldsymbol{x} \right)}{\partial \boldsymbol{x}_{n\times 1}}
=\left[ \frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\cdots,\frac{\partial f}{\partial x_n} \right]^{\mathrm{T}}
$$

运算法则：

$$
\begin{aligned}
\frac{\partial c}{\partial \boldsymbol{x}} &= 0\\
\frac{\partial \left[ c_1f\left( \boldsymbol{x} \right)+c_2g\left( \boldsymbol{x} \right) \right]}{\partial \boldsymbol{x}} &=c_1\frac{\partial f\left( \boldsymbol{x} \right)}{\partial \boldsymbol{x}}+c_2\frac{\partial g\left( \boldsymbol{x} \right)}{\partial \boldsymbol{x}} \\
\frac{\partial \left[ f\left( \boldsymbol{x} \right) g\left( \boldsymbol{x} \right)\right]}{\partial \boldsymbol{x}} &=\frac{\partial f\left( \boldsymbol{x} \right)}{\partial \boldsymbol{x}}g\left( \boldsymbol{x} \right)+f\left( \boldsymbol{x} \right)\frac{\partial g\left( \boldsymbol{x} \right)}{\partial \boldsymbol{x}} \\
\frac{\partial \left[ \frac{\partial f\left( \boldsymbol{x} \right)}{\partial g\left( \boldsymbol{x} \right)} \right]}{\partial \boldsymbol{x}} &=\frac{1}{g^2\left( \boldsymbol{x} \right)}\left[ \frac{\partial f\left( \boldsymbol{x} \right)}{\partial \boldsymbol{x}}g\left( \boldsymbol{x} \right)-f\left( \boldsymbol{x} \right)\frac{\partial f\left( \boldsymbol{x} \right)}{\partial \boldsymbol{x}} \right] \\
\end{aligned}
$$

常用公式，假设 $\boldsymbol{a}_{n\times 1},\boldsymbol{b}_{n\times 1}$ 为 constant vector, $\mathbf{A}_{n\times n}$ 为 constant matrix，则：

$$
\begin{aligned}
\frac{\partial \left( \boldsymbol{x}^{\mathrm{T}}\boldsymbol{a} \right)}{\partial \boldsymbol{x}} &=\frac{\partial }{\partial x} \\
&= \\
&= \\
&= \\
\end{aligned}
$$

### 矩阵变元的实值标量函数 $f\left( \boldsymbol{X} \right)$

### 矩阵函数的微分

### 利用矩阵微分求导

## 内积、外积与向量积

### 内积（标量积）

$$
\boldsymbol{a} \cdot\boldsymbol{b}=\left<\boldsymbol{a},\boldsymbol{b}\right> = \left\| \boldsymbol{a} \right\| \left\| \boldsymbol{b} \right\| \cos \theta
$$

### 叉积（向量积）

<figure id="fig:20250628235654434"><img src="https://raw.githubusercontent.com/HikariOri/image/main/20250628235654434.png" alt="20250628235654434"/><figcaption>叉积</figcaption></figure>

### 外积（张量积）

### 克罗内克（Kronecker）积

## 内积空间
